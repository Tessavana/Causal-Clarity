{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6318b9-8f4d-4238-b6a8-6ed57d15629c",
   "metadata": {},
   "source": [
    "## 2-level classification dataset creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b7486218-e961-49ea-8608-d59ae773de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('./df_review_orig.csv')\n",
    "\n",
    "# Rename columns for ease of use, based on exact names observed in the DataFrame\n",
    "df.rename(columns={\n",
    "    'Sentence':'sentence',\n",
    "    'Label':'label',\n",
    "}, inplace=True)\n",
    "\n",
    "essential_columns = [\n",
    "    'sentence', 'label'                            \n",
    "]\n",
    "\n",
    "# Filter the DataFrame to include only rows with non-NA values in these columns\n",
    "haber_data = df.dropna(subset=essential_columns)\n",
    "\n",
    "# Select only the essential columns and drop rows with any NaN values in these columns\n",
    "haber_data = haber_data[essential_columns].dropna()\n",
    "\n",
    "# Map the labels to their corresponding categories\n",
    "label_mapping = {\n",
    "    \"None: The linking sentence does not imply in any way a causal relationship was identified.\": \"0\",\n",
    "    \"Weak: The linking sentence might imply a causal relationship was identified, but it is unclear or possible to come to that conclusion in the absence of any causal inference.\": \"1\",\n",
    "    \"Moderate: The linking sentence mostly implies a causal relationship was identified, but it is unclear or possible to come to that conclusion in the absence of any causal inference.\": \"2\",\n",
    "    \"Strong: The linking sentence clearly implies that causality had been identified.\": \"3\"\n",
    "}\n",
    "\n",
    "# Apply the label mapping\n",
    "haber_data['label'] = haber_data['label'].map(label_mapping)\n",
    "\n",
    "# Save the cleaned and filtered DataFrame\n",
    "haber_data.to_csv('haber_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e21b601c-89e1-4388-8b76-f0737d5c16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution per source:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>haber</th>\n",
       "      <td>592</td>\n",
       "      <td>1045</td>\n",
       "      <td>1059</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>press_release</th>\n",
       "      <td>484</td>\n",
       "      <td>738</td>\n",
       "      <td>284</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pubmed</th>\n",
       "      <td>1353</td>\n",
       "      <td>494</td>\n",
       "      <td>213</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssc</th>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>168</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label             0     1     2    3\n",
       "source                              \n",
       "haber           592  1045  1059  706\n",
       "press_release   484   738   284  567\n",
       "pubmed         1353   494   213  995\n",
       "ssc               4    56   168  197"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the datasets\n",
    "haber_data = pd.read_csv('./haber_data.csv')\n",
    "press_release_data = pd.read_excel('./Press_release_data.xlsx')\n",
    "pubmed_data = pd.read_csv('./Pubmed data.csv')\n",
    "ssc_data=pd.read_excel('./labeled_ssc_data.xlsx')\n",
    "\n",
    "# Add a source column to identify where each entry came from\n",
    "haber_data['source'] = 'haber'\n",
    "press_release_data['source'] = 'press_release'\n",
    "pubmed_data['source'] = 'pubmed'\n",
    "ssc_data['source'] = 'ssc'\n",
    "\n",
    "# Combine all datasets into one DataFrame\n",
    "combined_data = pd.concat([haber_data, press_release_data, pubmed_data, ssc_data], ignore_index=True)\n",
    "combined_data = combined_data.drop_duplicates()\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "combined_data_shuffled = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the shuffled combined dataset to a new CSV file in the current directory\n",
    "output_file_path = './full dataset 4 classes.csv'\n",
    "combined_data_shuffled.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Compute the label distribution per source and per label\n",
    "label_distribution_per_source = combined_data_shuffled.groupby(['source', 'label']).size().unstack(fill_value=0)\n",
    "\n",
    "# Print the label distribution per source\n",
    "print(\"\\nLabel distribution per source:\")\n",
    "display(label_distribution_per_source)\n",
    "\n",
    "# Optionally save the label distribution to a CSV file\n",
    "label_distribution_file_path = './label_distribution_per_source.csv'\n",
    "label_distribution_per_source.to_csv(label_distribution_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1ea06721-205e-4dea-bc01-f8749c5f0092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Impact of bariatric surgery on health depends ...</td>\n",
       "      <td>3</td>\n",
       "      <td>press_release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Three quarters of pregnant women take sick lea...</td>\n",
       "      <td>3</td>\n",
       "      <td>press_release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>surgeon specialization accounted for 9% (coron...</td>\n",
       "      <td>2</td>\n",
       "      <td>haber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" We confirmed the robust association of MTNR1...</td>\n",
       "      <td>3</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In addition, higher order interaction analyses...</td>\n",
       "      <td>0</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8950</th>\n",
       "      <td>To our knowledge, this is the first study to i...</td>\n",
       "      <td>0</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8951</th>\n",
       "      <td>Short intervals between pregnancies result in ...</td>\n",
       "      <td>3</td>\n",
       "      <td>press_release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8952</th>\n",
       "      <td>Beneficial effects of surgery for epilepsy are...</td>\n",
       "      <td>3</td>\n",
       "      <td>press_release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8953</th>\n",
       "      <td>Longitudinal evidence of telomere length track...</td>\n",
       "      <td>2</td>\n",
       "      <td>haber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8954</th>\n",
       "      <td>Our study highlights the role of total cholest...</td>\n",
       "      <td>1</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8955 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label         source\n",
       "0     Impact of bariatric surgery on health depends ...      3  press_release\n",
       "1     Three quarters of pregnant women take sick lea...      3  press_release\n",
       "2     surgeon specialization accounted for 9% (coron...      2          haber\n",
       "3     \" We confirmed the robust association of MTNR1...      3         pubmed\n",
       "4     In addition, higher order interaction analyses...      0         pubmed\n",
       "...                                                 ...    ...            ...\n",
       "8950  To our knowledge, this is the first study to i...      0         pubmed\n",
       "8951  Short intervals between pregnancies result in ...      3  press_release\n",
       "8952  Beneficial effects of surgery for epilepsy are...      3  press_release\n",
       "8953  Longitudinal evidence of telomere length track...      2          haber\n",
       "8954  Our study highlights the role of total cholest...      1         pubmed\n",
       "\n",
       "[8955 rows x 3 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "aada028c-3d07-4e49-a21e-53ed47547faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ===\n",
      "label             1     2     3\n",
      "source                         \n",
      "haber           539   753   345\n",
      "press_release   353   184   260\n",
      "pubmed          273   153   465\n",
      "ssc              42   117   137\n",
      "All            1207  1207  1207\n",
      "\n",
      "=== Validation ===\n",
      "label            1    2    3\n",
      "source                      \n",
      "haber           68  112   48\n",
      "press_release   62   31   42\n",
      "pubmed          39    9   66\n",
      "ssc              3   20   16\n",
      "All            172  172  172\n",
      "\n",
      "=== Test ===\n",
      "label            1    2    3\n",
      "source                      \n",
      "haber          153  194   89\n",
      "press_release  116   69   80\n",
      "pubmed          65   51  132\n",
      "ssc             11   31   44\n",
      "All            345  345  345\n",
      "\n",
      "=== SSC Test ===\n",
      "label    1   2   3\n",
      "source            \n",
      "ssc     11  31  44\n",
      "\n",
      "Train size: 3621\n",
      "Val size:   516\n",
      "Test size:  1035\n",
      "SSC Test size:  86\n",
      "\n",
      "=== Combined ===\n",
      "label             1     2     3\n",
      "source                         \n",
      "haber           760  1059   482\n",
      "press_release   531   284   382\n",
      "pubmed          377   213   663\n",
      "ssc              56   168   197\n",
      "All            1724  1724  1724\n",
      "\n",
      "Combined size: 5172\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Drop all sentences with label 0\n",
    "combined_data_shuffled = combined_data_shuffled[\n",
    "    combined_data_shuffled['label'].isin([1, 2, 3])\n",
    "]\n",
    "\n",
    "# 2) Split the original dataset into training (70%) and remaining (30%)\n",
    "train_df, remaining_df = train_test_split(\n",
    "    combined_data_shuffled,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=combined_data_shuffled['label']\n",
    ")\n",
    "\n",
    "# 3) Split the remaining 30% into validation (10%) and test (20%)\n",
    "val_df, test_df = train_test_split(\n",
    "    remaining_df,\n",
    "    test_size=2/3,  # so overall test = 20%, val = 10%\n",
    "    random_state=42,\n",
    "    stratify=remaining_df['label']\n",
    ")\n",
    "\n",
    "# Capture duplicates before dropping\n",
    "train_duplicates = train_df[train_df.duplicated(keep='first')]\n",
    "val_duplicates = val_df[val_df.duplicated(keep='first')]\n",
    "test_duplicates = test_df[test_df.duplicated(keep='first')]\n",
    "\n",
    "# 4) Drop duplicates\n",
    "train_df = train_df.drop_duplicates()\n",
    "val_df   = val_df.drop_duplicates()\n",
    "test_df  = test_df.drop_duplicates()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Function: pure undersampling of non-SSC to achieve perfect label balance.\n",
    "#\n",
    "#  - Keep all SSC rows for each label (no removal).\n",
    "#  - For each label (1,2,3):\n",
    "#    * Let total_label = (ssc_label_count + non_ssc_label_count).\n",
    "#    * We'll find target = min(total_label(1), total_label(2), total_label(3)).\n",
    "#    * For each label, we reduce non-SSC so that the final label count = target.\n",
    "#  - If ssc_label_count for a label is already larger than 'target',\n",
    "#    pure undersampling won't fix it. That situation is impossible to balance\n",
    "#    without dropping SSC or oversampling other labels.\n",
    "# ----------------------------------------------------------------------\n",
    "def undersample_non_ssc_for_perfect_balance(df):\n",
    "    # Separate SSC from non-SSC\n",
    "    ssc_df     = df[df['source'] == 'ssc']\n",
    "    non_ssc_df = df[df['source'] != 'ssc']\n",
    "    \n",
    "    # Count how many SSC rows for each label\n",
    "    ssc_label_counts = ssc_df['label'].value_counts().reindex([1,2,3], fill_value=0)\n",
    "    # Count how many non-SSC rows for each label\n",
    "    non_ssc_label_counts = non_ssc_df['label'].value_counts().reindex([1,2,3], fill_value=0)\n",
    "    \n",
    "    # Current total per label = (SSC count + non-SSC count)\n",
    "    current_totals = ssc_label_counts + non_ssc_label_counts\n",
    "    \n",
    "    # We'll set the final \"target\" to be the *smallest* of these 3 totals\n",
    "    # so all labels 1,2,3 end up with the same final count\n",
    "    target = current_totals.min()\n",
    "    \n",
    "    # If for any label the ssc_label_counts[label] > target,\n",
    "    # it's impossible to fix with only undersampling non-SSC,\n",
    "    # since we won't remove or reduce SSC. We'll just raise an error or warning.\n",
    "    for label_val in [1, 2, 3]:\n",
    "        if ssc_label_counts[label_val] > target:\n",
    "            # Can't fix that with pure undersampling, because that\n",
    "            # label already has more SSC than 'target'.\n",
    "            print(f\"WARNING: Label {label_val} in SSC alone \"\n",
    "                  f\"exceeds target {target}. \"\n",
    "                  f\"Cannot achieve perfect balance without \"\n",
    "                  f\"dropping some SSC or oversampling others.\")\n",
    "            # We'll return df unchanged or you might choose to raise an exception\n",
    "            return df\n",
    "    \n",
    "    # Now we can safely undersample non-SSC for each label to meet 'target'\n",
    "    balanced_non_ssc_parts = []\n",
    "    \n",
    "    for label_val in [1, 2, 3]:\n",
    "        # Subset of non-SSC for this label\n",
    "        label_subset = non_ssc_df[non_ssc_df['label'] == label_val]\n",
    "        \n",
    "        # How many from SSC do we have?\n",
    "        ssc_count_for_label = ssc_label_counts[label_val]\n",
    "        \n",
    "        # So we want: final label count = target\n",
    "        # That means non-SSC portion must be: target - ssc_count_for_label\n",
    "        desired_non_ssc = target - ssc_count_for_label\n",
    "        \n",
    "        if len(label_subset) > desired_non_ssc:\n",
    "            # Undersample\n",
    "            label_subset = label_subset.sample(n=desired_non_ssc, random_state=42)\n",
    "        \n",
    "        # If label_subset is already <= desired_non_ssc, we keep it as is\n",
    "        # (no need to oversample).\n",
    "        \n",
    "        balanced_non_ssc_parts.append(label_subset)\n",
    "    \n",
    "    # Combine undersampled (or as-is) non-SSC from all labels\n",
    "    balanced_non_ssc_df = pd.concat(balanced_non_ssc_parts, ignore_index=True)\n",
    "    \n",
    "    # Finally, combine with all SSC rows\n",
    "    final_df = pd.concat([ssc_df, balanced_non_ssc_df], ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Balance each split by pure undersampling non-SSC\n",
    "# ---------------------------------------------------------\n",
    "balanced_train_df = undersample_non_ssc_for_perfect_balance(train_df)\n",
    "balanced_val_df   = undersample_non_ssc_for_perfect_balance(val_df)\n",
    "balanced_test_df  = undersample_non_ssc_for_perfect_balance(test_df)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) Create the SSC Test Set (from test set only)\n",
    "# ---------------------------------------------------------\n",
    "# Extract SSC data from the balanced test set\n",
    "ssc_test_df = balanced_test_df[balanced_test_df['source'] == 'ssc']\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) Calculate label distributions for train, val, test\n",
    "# ---------------------------------------------------------\n",
    "train_label_dist = balanced_train_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "val_label_dist   = balanced_val_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "test_label_dist  = balanced_test_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "test_ssc_label_dist  = ssc_test_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "\n",
    "\n",
    "train_label_dist.loc['All'] = train_label_dist.sum()\n",
    "val_label_dist.loc['All']   = val_label_dist.sum()\n",
    "test_label_dist.loc['All']  = test_label_dist.sum()\n",
    "\n",
    "\n",
    "print(\"=== Training ===\")\n",
    "print(train_label_dist)\n",
    "print(\"\\n=== Validation ===\")\n",
    "print(val_label_dist)\n",
    "print(\"\\n=== Test ===\")\n",
    "print(test_label_dist)\n",
    "print(\"\\n=== SSC Test ===\")\n",
    "print(test_ssc_label_dist)\n",
    "\n",
    "print(f\"\\nTrain size: {len(balanced_train_df)}\")\n",
    "print(f\"Val size:   {len(balanced_val_df)}\")\n",
    "print(f\"Test size:  {len(balanced_test_df)}\")\n",
    "print(f\"SSC Test size:  {len(ssc_test_df)}\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8) Save the Balanced Splits to CSV\n",
    "# ---------------------------------------------------------\n",
    "balanced_train_df.to_csv('./3_balanced_trainv2.csv', index=False)\n",
    "balanced_val_df.to_csv('./3_balanced_valv2.csv', index=False)\n",
    "balanced_test_df.to_csv('./3_balanced_testv2.csv', index=False)\n",
    "# Save the SSC test set to a CSV file\n",
    "ssc_test_df.to_csv('./3_ssc_testv2.csv', index=False)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9) Check final overall distribution if needed\n",
    "# ---------------------------------------------------------\n",
    "combined_balanced_df = pd.concat([balanced_train_df, balanced_val_df, balanced_test_df], ignore_index=True)\n",
    "combined_label_dist  = combined_balanced_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "combined_label_dist.loc['All'] = combined_label_dist.sum()\n",
    "print(\"\\n=== Combined ===\")\n",
    "print(combined_label_dist)\n",
    "print(f\"\\nCombined size: {len(combined_balanced_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2f372e17-eeed-43c1-a8b7-0a07c990e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Duplicates Count per Source ===\n",
      "\n",
      "Train Set Duplicates per Source:\n",
      "source\n",
      "haber     326\n",
      "pubmed      1\n",
      "ssc        14\n",
      "Name: sentence, dtype: int64\n",
      "\n",
      "Validation Set Duplicates per Source:\n",
      "source\n",
      "haber    14\n",
      "Name: sentence, dtype: int64\n",
      "\n",
      "Test Set Duplicates per Source:\n",
      "source\n",
      "haber    20\n",
      "ssc       2\n",
      "Name: sentence, dtype: int64\n",
      "\n",
      "=== Duplicate Sentences Removed ===\n",
      "\n",
      "Train Duplicates Sentences:\n",
      "     source                                           sentence\n",
      "819   haber  Nonalcoholic fatty liver disease was independe...\n",
      "1120  haber  These results provide clear evidence for a Dep...\n",
      "6642  haber  Liraglutide added to high-dose insulin therapy...\n",
      "5710  haber  Chronic inflammation, as ascertained by repeat...\n",
      "7643  haber  Rates of diagnosis and treatment of ADHD are h...\n",
      "...     ...                                                ...\n",
      "8734  haber  Only DASI scores were associated with predicti...\n",
      "1281  haber  The risk of non-fatal venous thromboembolism a...\n",
      "8641  haber  First, second and third trimester fetal growth...\n",
      "1887  haber  A higher consumption of processed meat was ass...\n",
      "1691  haber  Web-based vaccination information sent to preg...\n",
      "\n",
      "[341 rows x 2 columns]\n",
      "\n",
      "Validation Duplicates Sentences:\n",
      "     source                                           sentence\n",
      "8724  haber  There is no association between risk of early ...\n",
      "573   haber  Exposure to household disinfectants was associ...\n",
      "7379  haber  Overall, findings suggest that individual- and...\n",
      "4891  haber  Exposure to household disinfectants was associ...\n",
      "1378  haber  Overall, findings suggest that individual- and...\n",
      "1086  haber  Living in a household with a prescription opio...\n",
      "8793  haber  Although ambient air pollution during pregnanc...\n",
      "7225  haber  Living in a household with a prescription opio...\n",
      "1326  haber  In this study, ACT was consistently associated...\n",
      "4013  haber  Although ambient air pollution during pregnanc...\n",
      "6042  haber  There is no association between risk of early ...\n",
      "4969  haber  Sleep deficiency due to either sleep apnea or ...\n",
      "7410  haber  Sleep deficiency due to either sleep apnea or ...\n",
      "5983  haber  In this study, ACT was consistently associated...\n",
      "\n",
      "Test Duplicates Sentences:\n",
      "     source                                           sentence\n",
      "2554  haber  Among women who worked as registered nurses, l...\n",
      "8191  haber  An increase in ultraprocessed foods consumptio...\n",
      "3494  haber  Not wearing a helmet while cycling is associat...\n",
      "509     ssc  We argue and show that an important reason why...\n",
      "231   haber  An increase in BMI appeared to have a predicti...\n",
      "932   haber  Among a cohort of participants with severe obe...\n",
      "1848  haber  Not wearing a helmet while cycling is associat...\n",
      "3388  haber  An increase in BMI appeared to have a predicti...\n",
      "3118  haber  Minors who tried combustible cigarettes were m...\n",
      "3372  haber  Among a cohort of participants with severe obe...\n",
      "2799  haber  Adolescents and parents who lived in towns wit...\n",
      "4162  haber  We report high rates of metabolic bone disorde...\n",
      "3564  haber  Resting heart rate is a potent predictor of th...\n",
      "4783  haber  An increase in ultraprocessed foods consumptio...\n",
      "2542  haber  We report high rates of metabolic bone disorde...\n",
      "6008  haber  Minors who tried combustible cigarettes were m...\n",
      "1050  haber  Healthcare professional weight loss advice app...\n",
      "1898    ssc  We argue and show that an important reason why...\n",
      "8752  haber  Among women who worked as registered nurses, l...\n",
      "1447  haber  Healthcare professional weight loss advice app...\n",
      "5738  haber  Adolescents and parents who lived in towns wit...\n",
      "3060  haber  Resting heart rate is a potent predictor of th...\n",
      "\n",
      "=== Training ===\n",
      "label             1     2     3\n",
      "source                         \n",
      "haber           482   677   297\n",
      "press_release   347   184   243\n",
      "pubmed          257   152   450\n",
      "ssc              40   113   136\n",
      "All            1126  1126  1126\n",
      "\n",
      "=== Validation ===\n",
      "label            1    2    3\n",
      "source                      \n",
      "haber           68  109   47\n",
      "press_release   62   31   41\n",
      "pubmed          36    9   65\n",
      "ssc              3   20   16\n",
      "All            169  169  169\n",
      "\n",
      "=== Test ===\n",
      "label            1    2    3\n",
      "source                      \n",
      "haber          145  191   88\n",
      "press_release  119   69   79\n",
      "pubmed          67   51  132\n",
      "ssc             11   31   43\n",
      "All            342  342  342\n",
      "\n",
      "=== SSC Test ===\n",
      "label    1   2   3\n",
      "source            \n",
      "ssc     11  31  43\n",
      "\n",
      "Train size: 3378\n",
      "Val size:   507\n",
      "Test size:  1026\n",
      "SSC Test size:  85\n",
      "\n",
      "=== Combined ===\n",
      "label             1     2     3\n",
      "source                         \n",
      "haber           695   977   432\n",
      "press_release   528   284   363\n",
      "pubmed          360   212   647\n",
      "ssc              54   164   195\n",
      "All            1637  1637  1637\n",
      "\n",
      "Combined size: 4911\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Drop all sentences with label 0\n",
    "combined_data_shuffled = combined_data_shuffled[\n",
    "    combined_data_shuffled['label'].isin([1, 2, 3])\n",
    "]\n",
    "\n",
    "# 2) Split the original dataset into training (70%) and remaining (30%)\n",
    "train_df, remaining_df = train_test_split(\n",
    "    combined_data_shuffled,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=combined_data_shuffled['label']\n",
    ")\n",
    "\n",
    "# 3) Split the remaining 30% into validation (10%) and test (20%)\n",
    "val_df, test_df = train_test_split(\n",
    "    remaining_df,\n",
    "    test_size=2/3,  # so overall test = 20%, val = 10%\n",
    "    random_state=42,\n",
    "    stratify=remaining_df['label']\n",
    ")\n",
    "\n",
    "# Count duplicates per 'sentence' before dropping them\n",
    "train_duplicates = train_df[train_df.duplicated(subset='sentence', keep=False)]\n",
    "val_duplicates = val_df[val_df.duplicated(subset='sentence', keep=False)]\n",
    "test_duplicates = test_df[test_df.duplicated(subset='sentence', keep=False)]\n",
    "\n",
    "# 4) Count the number of duplicates per source\n",
    "train_duplicates_count = train_duplicates.groupby('source')['sentence'].count()\n",
    "val_duplicates_count = val_duplicates.groupby('source')['sentence'].count()\n",
    "test_duplicates_count = test_duplicates.groupby('source')['sentence'].count()\n",
    "\n",
    "# Drop duplicates\n",
    "train_df = train_df.drop_duplicates(subset='sentence')\n",
    "val_df   = val_df.drop_duplicates(subset='sentence')\n",
    "test_df  = test_df.drop_duplicates(subset='sentence')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Function: pure undersampling of non-SSC to achieve perfect label balance.\n",
    "# ----------------------------------------------------------------------\n",
    "def undersample_non_ssc_for_perfect_balance(df):\n",
    "    ssc_df     = df[df['source'] == 'ssc']\n",
    "    non_ssc_df = df[df['source'] != 'ssc']\n",
    "    \n",
    "    ssc_label_counts = ssc_df['label'].value_counts().reindex([1,2,3], fill_value=0)\n",
    "    non_ssc_label_counts = non_ssc_df['label'].value_counts().reindex([1,2,3], fill_value=0)\n",
    "    \n",
    "    current_totals = ssc_label_counts + non_ssc_label_counts\n",
    "    target = current_totals.min()\n",
    "    \n",
    "    for label_val in [1, 2, 3]:\n",
    "        if ssc_label_counts[label_val] > target:\n",
    "            print(f\"WARNING: Label {label_val} in SSC alone \"\n",
    "                  f\"exceeds target {target}. \"\n",
    "                  f\"Cannot achieve perfect balance without \"\n",
    "                  f\"dropping some SSC or oversampling others.\")\n",
    "            return df\n",
    "    \n",
    "    balanced_non_ssc_parts = []\n",
    "    \n",
    "    for label_val in [1, 2, 3]:\n",
    "        label_subset = non_ssc_df[non_ssc_df['label'] == label_val]\n",
    "        ssc_count_for_label = ssc_label_counts[label_val]\n",
    "        desired_non_ssc = target - ssc_count_for_label\n",
    "        \n",
    "        if len(label_subset) > desired_non_ssc:\n",
    "            label_subset = label_subset.sample(n=desired_non_ssc, random_state=42)\n",
    "        \n",
    "        balanced_non_ssc_parts.append(label_subset)\n",
    "    \n",
    "    balanced_non_ssc_df = pd.concat(balanced_non_ssc_parts, ignore_index=True)\n",
    "    final_df = pd.concat([ssc_df, balanced_non_ssc_df], ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Balance each split by pure undersampling non-SSC\n",
    "# ---------------------------------------------------------\n",
    "balanced_train_df = undersample_non_ssc_for_perfect_balance(train_df)\n",
    "balanced_val_df   = undersample_non_ssc_for_perfect_balance(val_df)\n",
    "balanced_test_df  = undersample_non_ssc_for_perfect_balance(test_df)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) Create the SSC Test Set (from test set only)\n",
    "# ---------------------------------------------------------\n",
    "ssc_test_df = balanced_test_df[balanced_test_df['source'] == 'ssc']\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) Calculate label distributions for train, val, test\n",
    "# ---------------------------------------------------------\n",
    "train_label_dist = balanced_train_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "val_label_dist   = balanced_val_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "test_label_dist  = balanced_test_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "test_ssc_label_dist  = ssc_test_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "\n",
    "train_label_dist.loc['All'] = train_label_dist.sum()\n",
    "val_label_dist.loc['All']   = val_label_dist.sum()\n",
    "test_label_dist.loc['All']  = test_label_dist.sum()\n",
    "\n",
    "# Print duplicates count per source\n",
    "print(\"=== Duplicates Count per Source ===\")\n",
    "print(f\"\\nTrain Set Duplicates per Source:\")\n",
    "print(train_duplicates_count)\n",
    "\n",
    "print(f\"\\nValidation Set Duplicates per Source:\")\n",
    "print(val_duplicates_count)\n",
    "\n",
    "print(f\"\\nTest Set Duplicates per Source:\")\n",
    "print(test_duplicates_count)\n",
    "\n",
    "# Print duplicate sentences that were removed\n",
    "print(\"\\n=== Duplicate Sentences Removed ===\")\n",
    "print(\"\\nTrain Duplicates Sentences:\")\n",
    "print(train_duplicates[['source', 'sentence']])\n",
    "\n",
    "print(\"\\nValidation Duplicates Sentences:\")\n",
    "print(val_duplicates[['source', 'sentence']])\n",
    "\n",
    "print(\"\\nTest Duplicates Sentences:\")\n",
    "print(test_duplicates[['source', 'sentence']])\n",
    "\n",
    "print(\"\\n=== Training ===\")\n",
    "print(train_label_dist)\n",
    "print(\"\\n=== Validation ===\")\n",
    "print(val_label_dist)\n",
    "print(\"\\n=== Test ===\")\n",
    "print(test_label_dist)\n",
    "print(\"\\n=== SSC Test ===\")\n",
    "print(test_ssc_label_dist)\n",
    "\n",
    "print(f\"\\nTrain size: {len(balanced_train_df)}\")\n",
    "print(f\"Val size:   {len(balanced_val_df)}\")\n",
    "print(f\"Test size:  {len(balanced_test_df)}\")\n",
    "print(f\"SSC Test size:  {len(ssc_test_df)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8) Save the Balanced Splits to CSV\n",
    "# ---------------------------------------------------------\n",
    "balanced_train_df.to_csv('./3_balanced_trainv2.csv', index=False)\n",
    "balanced_val_df.to_csv('./3_balanced_valv2.csv', index=False)\n",
    "balanced_test_df.to_csv('./3_balanced_testv2.csv', index=False)\n",
    "ssc_test_df.to_csv('./3_ssc_testv2.csv', index=False)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9) Check final overall distribution if needed\n",
    "# ---------------------------------------------------------\n",
    "combined_balanced_df = pd.concat([balanced_train_df, balanced_val_df, balanced_test_df], ignore_index=True)\n",
    "combined_label_dist  = combined_balanced_df.groupby(['source','label']).size().unstack(fill_value=0)\n",
    "combined_label_dist.loc['All'] = combined_label_dist.sum()\n",
    "print(\"\\n=== Combined ===\")\n",
    "print(combined_label_dist)\n",
    "print(f\"\\nCombined size: {len(combined_balanced_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd044300-8ac0-40a5-9eec-476ae56adcb3",
   "metadata": {},
   "source": [
    "## 2-level classification dataset creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0e60bd93-1fdd-422f-a5ed-ab3a944bcceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in ssc_train.csv: ['text', 'label']\n",
      "Columns in ssc_val.csv: ['text', 'label']\n",
      "Columns in ssc_test (1).csv: ['text', 'label']\n",
      "\n",
      "Combined dataset saved as: ./ssc_data_all.csv\n",
      "\n",
      "Total label distribution for the combined social science dataset:\n",
      "label\n",
      "1    529\n",
      "0    529\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define dataset path\n",
    "data_path = './'\n",
    "\n",
    "# List of social science dataset files\n",
    "social_science_files = ['ssc_train.csv', 'ssc_val.csv', 'ssc_test (1).csv']\n",
    "\n",
    "# Store cleaned DataFrames\n",
    "ssc_dataframes = []\n",
    "\n",
    "for file in social_science_files:\n",
    "    file_path = os.path.join(data_path, file)\n",
    "\n",
    "    # Ensure file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: {file} not found. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Print existing columns for debugging\n",
    "    print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "\n",
    "    # Check if expected columns exist, else print available ones\n",
    "    if 'text' in df.columns and 'label' in df.columns:\n",
    "        df = df[['text', 'label']].copy()  # Keep only necessary columns\n",
    "        df.rename(columns={'text': 'sentence'}, inplace=True)\n",
    "    elif 'sentence' in df.columns and 'label' in df.columns:\n",
    "        df = df[['sentence', 'label']].copy()  # Keep only necessary columns\n",
    "    else:\n",
    "        print(f\"Skipping {file}: Required columns ('text' or 'sentence', 'label') not found.\")\n",
    "        continue  # Skip this file if it doesn't have the required columns\n",
    "\n",
    "    # Convert labels to integer\n",
    "    df['label'] = df['label'].astype(int)\n",
    "\n",
    "    # Append cleaned DataFrame\n",
    "    ssc_dataframes.append(df)\n",
    "\n",
    "# Combine all social science datasets\n",
    "if ssc_dataframes:\n",
    "    combined_ssc_df = pd.concat(ssc_dataframes, ignore_index=True)\n",
    "\n",
    "    # Save the final combined dataset\n",
    "    output_file = os.path.join(data_path, 'ssc_data_all.csv')\n",
    "    combined_ssc_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nCombined dataset saved as: {output_file}\")\n",
    "\n",
    "    # Print total label distribution\n",
    "    ssc_total_label_distribution = combined_ssc_df['label'].value_counts()\n",
    "    print(\"\\nTotal label distribution for the combined social science dataset:\")\n",
    "    print(ssc_total_label_distribution)\n",
    "else:\n",
    "    print(\"No valid social science datasets found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "93a24d85-88fd-4172-b79c-6596655fa33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 duplicate sentences in altlex. Removing them.\n",
      "Found 15 duplicate sentences in altlex. Removing them.\n",
      "Found 174 duplicate sentences in because. Removing them.\n",
      "Found 3 duplicate sentences in because. Removing them.\n",
      "Found 1432 duplicate sentences in ctb. Removing them.\n",
      "Found 176 duplicate sentences in ctb. Removing them.\n",
      "Found 15423 duplicate sentences in esl2. Removing them.\n",
      "Found 1736 duplicate sentences in esl2. Removing them.\n",
      "Found 25 duplicate sentences in semeval. Removing them.\n",
      "Found 2 duplicate sentences in semeval. Removing them.\n",
      "Fixing labels in press_release: converting labels 1, 2, 3 → 1 and keeping label 0.\n",
      "Found 3 duplicate sentences in press_release. Removing them.\n",
      "Fixing labels in Pubmed data.csv: converting labels 1, 2, 3 → 1 and keeping label 0.\n",
      "Found 6 duplicate sentences in Pubmed data.csv. Removing them.\n",
      "Fixing labels in haber_data.csv: converting labels 1, 2, 3 → 1 and keeping label 0.\n",
      "Found 969 duplicate sentences in haber_data.csv. Removing them.\n",
      "\n",
      "### Dataset BEFORE Splitting ###\n",
      "label               0     1\n",
      "source                     \n",
      "Haber             592  2503\n",
      "Press_Release     484  1589\n",
      "Pubmed data.csv  1353  1702\n",
      "Ssc               529   529\n",
      "altlex            563   415\n",
      "because           101   321\n",
      "ctb              1925   276\n",
      "esl2             1072  1150\n",
      "semeval          9363  1327\n",
      "\n",
      "Total label distribution before splitting:\n",
      "label\n",
      "0    15982\n",
      "1     9812\n",
      "Name: count, dtype: int64\n",
      "\n",
      "### Dataset AFTER Splitting ###\n",
      "\n",
      "Train set distribution:\n",
      "label               0     1\n",
      "source                     \n",
      "Haber             243  1742\n",
      "Press_Release     207  1115\n",
      "Pubmed data.csv   577  1210\n",
      "Ssc               227   372\n",
      "altlex            258   289\n",
      "because            42   230\n",
      "ctb               814   195\n",
      "esl2              484   786\n",
      "semeval          4016   929\n",
      "\n",
      "Label distribution in Train set:\n",
      "label\n",
      "1    6868\n",
      "0    6868\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set distribution:\n",
      "label              0    1\n",
      "source                   \n",
      "Haber             42  226\n",
      "Press_Release     22  194\n",
      "Pubmed data.csv   84  152\n",
      "Ssc               35   45\n",
      "altlex            25   44\n",
      "because           10   28\n",
      "ctb              139   36\n",
      "esl2              49  119\n",
      "semeval          575  137\n",
      "\n",
      "Label distribution in Validation set:\n",
      "label\n",
      "1    981\n",
      "0    981\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distribution:\n",
      "label               0    1\n",
      "source                    \n",
      "Haber              74  535\n",
      "Press_Release      65  280\n",
      "Pubmed data.csv   164  340\n",
      "Ssc                55  112\n",
      "altlex             65   82\n",
      "because            10   63\n",
      "ctb               197   45\n",
      "esl2              135  245\n",
      "semeval          1198  261\n",
      "\n",
      "Label distribution in Test set:\n",
      "label\n",
      "1    1963\n",
      "0    1963\n",
      "Name: count, dtype: int64\n",
      "\n",
      "SSC Test set distribution:\n",
      "label    0   1\n",
      "source        \n",
      "Ssc     96  96\n",
      "\n",
      "Label distribution in SSC Test set:\n",
      "label\n",
      "1    96\n",
      "0    96\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define dataset path\n",
    "data_path = './'\n",
    "\n",
    "# List of dataset files with their sources\n",
    "file_names = {\n",
    "    'altlex': ['altlex_train.csv', 'altlex_test.csv'],\n",
    "    'because': ['because_train.csv', 'because_test.csv'],\n",
    "    'ctb': ['ctb_train.csv', 'ctb_test.csv'],\n",
    "    'esl2': ['esl2_train.csv', 'esl2_test.csv'],\n",
    "    'semeval': ['semeval2010t8_train.csv', 'semeval2010t8_test.csv']\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store dataframes and logs\n",
    "dataframes = {}\n",
    "duplicates_removed = {}\n",
    "nans_removed = {}\n",
    "\n",
    "# Function to check and rename necessary columns\n",
    "def check_and_rename_columns(df, file):\n",
    "    if 'text' in df.columns:\n",
    "        df.rename(columns={'text': 'sentence'}, inplace=True)\n",
    "    elif 'sentence' not in df.columns:\n",
    "        raise KeyError(f\"Expected column 'sentence' missing in file: {file}\")\n",
    "\n",
    "    if 'seq_label' in df.columns:\n",
    "        df.rename(columns={'seq_label': 'label'}, inplace=True)\n",
    "    elif 'category' in df.columns:\n",
    "        df.rename(columns={'category': 'label'}, inplace=True)\n",
    "\n",
    "    if 'label' not in df.columns:\n",
    "        raise KeyError(f\"Missing expected column 'label' in file: {file}. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to ensure labels are only 0 or 1\n",
    "def fix_labels(df, file):\n",
    "    if df['label'].dtype != int:\n",
    "        df['label'] = df['label'].astype(int)  \n",
    "\n",
    "    if df['label'].max() > 1:\n",
    "        print(f\"Fixing labels in {file}: converting labels 1, 2, 3 → 1 and keeping label 0.\")\n",
    "        df['label'] = df['label'].apply(lambda x: 1 if x in [1, 2, 3] else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to remove NaNs\n",
    "def remove_nans(df, file):\n",
    "    nan_rows = df[df[['sentence', 'label']].isnull().any(axis=1)]\n",
    "    if not nan_rows.empty:\n",
    "        print(f\"NaN values found in {file}. Dropping {len(nan_rows)} rows.\")\n",
    "        nans_removed[file] = len(nan_rows)\n",
    "        df = df.dropna(subset=['sentence', 'label'])\n",
    "    return df\n",
    "\n",
    "# Function to remove duplicates\n",
    "def remove_duplicates(df, source):\n",
    "    duplicate_rows = df[df.duplicated(subset=['sentence', 'label'], keep='first')]\n",
    "    if not duplicate_rows.empty:\n",
    "        print(f\"Found {len(duplicate_rows)} duplicate sentences in {source}. Removing them.\")\n",
    "        duplicates_removed[source] = len(duplicate_rows)\n",
    "        df = df.drop_duplicates(subset=['sentence', 'label'])\n",
    "    return df\n",
    "\n",
    "# Process each dataset\n",
    "for corpus, files in file_names.items():\n",
    "    corpus_dataframes = []\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: File {file} not found. Skipping...\")\n",
    "            continue  \n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = check_and_rename_columns(df, file)\n",
    "        df = fix_labels(df, file)\n",
    "        df = remove_nans(df, file)\n",
    "        df['source'] = corpus  \n",
    "        df = remove_duplicates(df, corpus)\n",
    "        corpus_dataframes.append(df)\n",
    "\n",
    "    if corpus_dataframes:\n",
    "        combined_corpus_df = pd.concat(corpus_dataframes, ignore_index=True)\n",
    "        combined_corpus_df = combined_corpus_df[combined_corpus_df['label'].isin([0, 1])]\n",
    "        dataframes[corpus] = combined_corpus_df\n",
    "    else:\n",
    "        print(f\"Warning: No valid data found for {corpus}. Skipping...\")\n",
    "\n",
    "# Process press release dataset (Excel file)\n",
    "press_release_data_path = './press_release_Data.xlsx'\n",
    "if os.path.exists(press_release_data_path):\n",
    "    press_release_df = pd.read_excel(press_release_data_path)\n",
    "    press_release_df = check_and_rename_columns(press_release_df, \"press_release\")\n",
    "    press_release_df = fix_labels(press_release_df, \"press_release\")\n",
    "    press_release_df = remove_nans(press_release_df, \"press_release\")\n",
    "    press_release_df = remove_duplicates(press_release_df, \"press_release\")\n",
    "    press_release_df['source'] = 'Press_Release'\n",
    "else:\n",
    "    print(\"Warning: Press release dataset not found. Skipping...\")\n",
    "    press_release_df = None\n",
    "\n",
    "# Load additional datasets\n",
    "additional_datasets = [\"Pubmed data.csv\", \"haber_data.csv\", \"ssc_data_all.csv\"]\n",
    "extra_dataframes = []\n",
    "\n",
    "for dataset in additional_datasets:\n",
    "    file_path = os.path.join(data_path, dataset)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = check_and_rename_columns(df, dataset)\n",
    "        df = fix_labels(df, dataset)\n",
    "        df = remove_nans(df, dataset)\n",
    "        df = remove_duplicates(df, dataset)\n",
    "        df['source'] = dataset.split(\"_\")[0].capitalize()\n",
    "        extra_dataframes.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: {dataset} not found. Skipping...\")\n",
    "\n",
    "# Combine all datasets\n",
    "final_combined_df = pd.concat(\n",
    "    [*dataframes.values(), *extra_dataframes, press_release_df] if press_release_df is not None else [*dataframes.values(), *extra_dataframes], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Print pre-split statistics\n",
    "print(\"\\n### Dataset BEFORE Splitting ###\")\n",
    "print(final_combined_df.groupby([\"source\", \"label\"]).size().unstack().fillna(0))\n",
    "print(\"\\nTotal label distribution before splitting:\")\n",
    "print(final_combined_df['label'].value_counts())\n",
    "\n",
    "# Split into train, validation, and test\n",
    "train_df, remaining_df = train_test_split(final_combined_df, test_size=0.3, random_state=42, stratify=final_combined_df['label'])\n",
    "validation_df, test_df = train_test_split(remaining_df, test_size=2/3, random_state=42, stratify=remaining_df['label'])\n",
    "\n",
    "# Function to balance dataset\n",
    "def balance_dataset(df):\n",
    "    causal_df = df[df['label'] == 1]\n",
    "    non_causal_df = df[df['label'] == 0]\n",
    "    min_count = min(len(causal_df), len(non_causal_df))\n",
    "    return pd.concat([causal_df.sample(n=min_count, random_state=42), non_causal_df.sample(n=min_count, random_state=42)])\n",
    "\n",
    "# Balance train, validation, and test sets\n",
    "balanced_train_df = balance_dataset(train_df)\n",
    "balanced_validation_df = balance_dataset(validation_df)\n",
    "balanced_test_df = balance_dataset(test_df)\n",
    "\n",
    "# ** Create Social Science Test Set **\n",
    "ssc_test_df = test_df[test_df['source'] == 'Ssc']  # Extract SSC data\n",
    "ssc_balanced_test_df = balance_dataset(ssc_test_df)  # Balance it\n",
    "\n",
    "# Save datasets with only 3 columns: sentence, label, source\n",
    "balanced_train_df[['sentence', 'label', 'source']].to_csv(data_path + '2_final_train_dataset.csv', index=False)\n",
    "balanced_validation_df[['sentence', 'label', 'source']].to_csv(data_path + '2_final_validation_dataset.csv', index=False)\n",
    "balanced_test_df[['sentence', 'label', 'source']].to_csv(data_path + '2_final_test_dataset.csv', index=False)\n",
    "ssc_balanced_test_df[['sentence', 'label', 'source']].to_csv(data_path + '2_final_social_science_test_dataset.csv', index=False)\n",
    "\n",
    "# Print post-split statistics\n",
    "print(\"\\n### Dataset AFTER Splitting ###\")\n",
    "for name, df in [(\"Train\", balanced_train_df), (\"Validation\", balanced_validation_df), (\"Test\", balanced_test_df), (\"SSC Test\", ssc_balanced_test_df)]:\n",
    "    print(f\"\\n{name} set distribution:\")\n",
    "    print(df.groupby([\"source\", \"label\"]).size().unstack().fillna(0))\n",
    "    print(f\"\\nLabel distribution in {name} set:\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "print(\"\\nFinal dataset saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a6973-b51f-4320-967e-35bf5183c833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
